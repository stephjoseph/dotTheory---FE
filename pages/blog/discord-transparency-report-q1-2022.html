<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <link
      href="https://assets-global.website-files.com/5f8dd67f8fdd6f51f0b50904/60ae916347747e71167e21cc_favicon.png"
      rel="shortcut icon"
      type="image/x-icon"
    />
    <link
      href="https://assets-global.website-files.com/5f8dd67f8fdd6f51f0b50904/5f91fae62cc821206588b837_Frame%20246.png"
      rel="apple-touch-icon"
    />
    <link rel="stylesheet" href="../../css/index.css" />
    <title>Discord Transparency Report: January - March 2022</title>
  </head>
  <body>
    <nav class="navbar">
      <a href=".">
        <img src="../../images/logo.svg" alt="back to home" />
      </a>
      <button type="button" class="nav-button">
        <img
          class="nav-button__hamburger"
          src="../../images/nav-button.svg"
          alt=""
        />
      </button>
      <div class="nav-links">
        <a class="nav-link nav-link__collection" href=""
          >Collections <img src="../../images/arrow-down.svg" alt=""
        /></a>
        <a class="nav-link" href="">Featured</a>
        <a class="nav-link" href="">Discord.com</a>
      </div>
    </nav>
    <header class="blog-header"></header>
    <main class="blog-main">
      <div class="blog__header">
        <div>
          <img
            class="blog__img"
            src="../../images/policy-and-safety-blog-1.png"
            alt=""
          />
        </div>
        <div class="blog__title-container">
          <a class="collection-title" href="">Policy & Safety</a>
          <a class="blog__title" href="/" `
            ><h1>Discord Transparency Report: January - March 2022</h1></a
          >
        </div>
      </div>
      <div class="blog__author-content-container">
        <div class="blog__author">
          <img
            src="../../images/discord-safety.png"
            alt=""
            class="blog__author-img"
          />

          <div class="blog__author-name-date-container">
            <div class="blog__author-name">Discord Safety</div>
            <div class="blog__date">June 30, 2022</div>
          </div>
        </div>
        <div class="blog__content">
          <p>
            We recognize that safety enables people to find belonging. That is
            why safety is one of Discord’s most important priorities and
            investments as a company. ‍
          </p>
          <p>
            Our Engineering, Data, and Product teams work with safety principles
            in mind when building products. Our Policy team takes a nuanced and
            sophisticated approach to developing our
            <a href="https://discord.com/guidelines">Community Guidelines</a>
            and forms strategic partnerships with academics, civil society,
            industry peers, and moderators to advance our collective
            understanding of online safety. Our Trust &amp; Safety team works
            with cutting-edge technology to detect and respond to abuse, both
            proactively and from users, moderators, and trusted third party
            reporters.
          </p>
          <p>
            This expansive effort and commitment to safety brings together
            around 15% of all Discord employees. We’re continuously expanding
            our efforts to help ensure that everyone can have a safer experience
            that allows for belonging on Discord.&nbsp;
          </p>
          <p>
            Transparency is one of the core values of our safety work at
            Discord. These Transparency Reports are a way to demonstrate our
            commitment and provide insight into the enormous effort that goes
            into keeping Discord a safe place for all.&nbsp;
          </p>
          <p>
            Below are a few recently added highlights that help make Discord
            better for everyone:
          </p>
          <h2>Report Structure Updates</h2>
          <p>
            As part of our ongoing commitment to be more transparent about the
            work that goes into keeping Discord safe, we’re doubling the number
            of Transparency Reports that we publish annually by moving from a
            semi-annual cadence to a quarterly cadence. This Report is our
            <a href="https://discord.com/tags/transparency-reports"
              >seventh since 2019</a
            >, and our first quarterly Report.&nbsp;
          </p>
          <p>
            In making this transition, we’re changing the reporting period
            reflected in the data. This Report covers the first quarter of 2022,
            from January to March and will compare the first quarter of 2022 to
            the fourth quarter of 2021, from October to December.
          </p>
          <p>
            We
            <a href="https://discord.com/blog/important-policy-updates"
              >recently updated our Community Guidelines</a
            >
            to reflect several new and clarified policies which went into effect
            at the end of March. Since this Report covers Discord’s safety
            efforts during the first quarter of 2022, the enforcement actions
            documented in this report are based on the previous version of our
            Community Guidelines.
          </p>
          <h2>Community Guidelines Enforcement</h2>
          <p>
            Discord publishes and maintains a comprehensive set of
            <a href="https://discord.com/guidelines">Community Guidelines</a>
            that explains what content and behavior is and isn’t allowed on
            Discord. We invest heavily in our proactive efforts to detect and
            remove abuse before it’s reported to us. Through advanced tooling,
            machine learning, specialized Safety teams tackling specific abuse
            situations, and partnering with experts outside the company, our
            team works to remove high-harm abuse before it is viewed or
            experienced by others.<br /><br />We encourage users, moderators,
            and trusted reporters to submit reports if they believe an account
            or server is violating our Community Guidelines. We review and
            analyze these reports to determine if the content or behavior
            violates our Community Guidelines. Depending on the severity, we may
            take a number of enforcement steps including but not limited to
            issuing warnings; removing content; temporarily or permanently
            disabling or removing the accounts and/or servers responsible; and
            potentially reporting them to law enforcement.&nbsp;
          </p>
          <p>
            This Report details the actions that Discord has taken on accounts
            and servers that have violated our Community Guidelines and
            <a href="https://discord.com/terms">Terms of Service</a>.<br />
          </p>
          <h2>Actions Taken on Accounts and Servers</h2>
          <h3>Account and Server Warnings</h3>
          <p>
            Discord provides warnings as an educational opportunity and as a
            support system to prevent future violations of our Community
            Guidelines. Warnings issued to accounts and servers are used
            frequently to correct problematic behavior that does not require
            immediate permanent removal from Discord. For some high-harm issues
            such as Child Sexual Abuse Material (CSAM) - a subcategory of Child
            Safety - we do not issue warnings but rather immediately disable the
            account, remove the content, and report the account to the
            <a href="https://www.missingkids.org/gethelpnow/cybertipline"
              >National Center for Missing and Exploited Children</a
            >
            (NCMEC).
          </p>
          <figure>
            <div>
              <img src="../../images/account-server-warnings.png" alt="" />
            </div>
            <figcaption>
              Alternatively, you can disable names being colored by roles
              entirely and stick to looking at a user’s profile to view their
              roles.&nbsp;
            </figcaption>
          </figure>
          <p>
            We issue two types of warnings for accounts: Individual Account
            Warnings are issued to individual users that engage in problematic
            behavior; Server Members Warnings target multiple members of a
            community and may be issued at either the time of warning a server,
            or when removing a server from Discord.
          </p>
          <p>
            Servers in violation of policies against Exploitative and
            Unsolicited Content, and Regulated or Illegal Activities are often
            removed without intermediary warning attempts. As a result, we see
            more normalized proportions of Server Members warned when combining
            the number of servers warned and servers removed for these
            categories.&nbsp;&nbsp;
          </p>
          <p>
            Warnings issued to Individual Accounts increased by 17% when
            compared to the previous quarter, to
            <strong>9,752 accounts</strong>, and the number of warnings issued
            to servers decreased by 14.5% to <strong>1,595 servers</strong>.
            Server Member Warnings were issued to
            <strong>2,086,826 accounts</strong>, an increase of 15.5% when
            compared to the previous quarter.
          </p>
          <h3>Accounts Disabled</h3>
          <figure>
            <div>
              <img src="../../images/accounts-disabled.png" alt="" />
            </div>
            <figcaption>
              An accessible text version of the number of accounts disabled by
              category chart is available
              <a
                href="https://github.com/DiscordBlog/Discord-Transparency-Report-January---March-2022/blob/main/Q1%202022%20Transparency%20Report%20-%20Accounts%20Disabled%20.csv"
                >on GitHub</a
              >.
            </figcaption>
          </figure>
          <p>
            We disabled <strong>1,054,358 accounts</strong> between January and
            March 2022 for policy violations not including spam, a decrease of
            6.5% from the 1,127,220 accounts disabled in the previous quarter.
          </p>
          <p>
            Child Safety was the largest category of accounts disabled with
            826,591 accounts, or 78.5% of accounts disabled. This is the result
            of our intentional and targeted set of efforts to detect and disable
            accounts violating our policies regarding Child Safety.&nbsp;
          </p>
          <p>
            Overall, <strong>26,017,742 accounts</strong> were disabled for spam
            or spam-related offenses. This is an increase of 31% from the
            previous quarter, when we disabled 19,794,949 spam accounts.&nbsp;
          </p>
          <h3>Servers Removed</h3>
          <figure>
            <div>
              <img src="../../images/servers-removed.png" alt="" />
            </div>
            <figcaption>
              The above chart breaks down the number of servers removed by
              category and if they were removed proactively or reactively. An
              accessible text version of this chart is available
              <a
                href="https://github.com/DiscordBlog/Discord-Transparency-Report-January---March-2022/blob/main/Q1%202022%20Transparency%20Report%20-%20Servers%20Removed.csv"
                >on GitHub</a
              >.
            </figcaption>
          </figure>
          <p>
            We removed <strong>40,009 servers</strong> between January and March
            2022, an increase of 61% from the 24,841 servers that we removed
            during the previous quarter. This increase was due to our faster
            detection and response time when removing problematic servers.&nbsp;
          </p>
          <p>
            As with actions taken on individual accounts, Child Safety was the
            largest category of servers removed.
          </p>
          <p>
            We dedicate significant resources to proactively removing servers
            before they’re reported to us. These efforts especially target the
            highest-harm categories on our platform.
          </p>
          <p>
            From the Child Safety category; CSAM and Sexual Content Regarding
            Minors (SCRM) or content that sexualizes and exploits minors but
            which is not reportable to NCMEC. From the Exploitative and
            Unsolicited Content category: Non-Consensual Pornography. From the
            Regulated and Illegal Activities category, Cybercrime, and lastly,
            Violent Extremism.&nbsp;
          </p>
          <figure>
            <div>
              <img src="../../images/high-arm-areas.png" alt="" />
            </div>
            <figcaption>
              The above chart shows a detailed breakdown of the high-harm areas
              that are proactively targeted.&nbsp; An accessible text version of
              this chart is available
              <a
                href="https://github.com/DiscordBlog/Discord-Transparency-Report-January---March-2022/blob/main/Q1%202022%20Transparency%20Report%20-%20High%20Harm%20Proactive%20.csv"
                >on GitHub</a
              >.
            </figcaption>
          </figure>
          <p>
            Overall, 62.5% of these high-harm servers were removed proactively
            compared to 42% in Q4 of 2021. We aim to increase the rate of
            proactive server removals, especially for these high-harm issues.
          </p>
          <h3>Appeals</h3>
          <figure>
            <div>
              <img src="../../images/appeals.png" alt="" />
            </div>
            <figcaption>
              The above chart shows the total percentage of accounts that
              submitted an appeal, and the percentage of those appeals that were
              granted. An accessible text version of this chart is available<a
                href="https://github.com/DiscordBlog/Discord-Transparency-Report-January---March-2022/blob/main/Q1%202022%20Transparency%20Report%20-%20Appeals.csv"
              >
                on GitHub</a
              >.
            </figcaption>
          </figure>
          <p>
            Discord allows users to appeal actions taken on their accounts if
            they feel that the enforcement was incorrect.
          </p>
          <p>
            We welcome appeals and take our users seriously when they make the
            effort to raise context or information that we may not have known
            when we made our decision. We review appeal requests and reinstate
            accounts if we determine that a mistake was made, or if we have good
            faith in the user’s appeal that they have recognized the violation
            made for a lower-harm issue and will abide by our Community
            Guidelines once back on Discord. We received<strong>
              118,444 appeals</strong
            >
            for issues not including spam, a 16% decrease in appeals from the
            previous quarter. <strong>2,365</strong> or 2% of users who
            submitted an appeal had their accounts reinstated.
          </p>
          <h2>Reports</h2>
          <h3>Reports Received By Category and Action Rates of Reports</h3>
          <figure>
            <div>
              <img src="../../images/reports.png" alt="" />
            </div>
            <figcaption>
              An accessible text version of the reports received chart is
              available
              <a
                href="https://github.com/DiscordBlog/Discord-Transparency-Report-January---March-2022/blob/main/Q1%202022%20Transparency%20Report%20-%20Report%20by%20Category%20and%20Action%20Rates.csv"
                >on GitHub</a
              >.
            </figcaption>
          </figure>
          <p>
            Reports received during the first quarter of 2022 increased
            marginally to <strong>178,414 </strong>when compared to 175,292
            reports during the fourth quarter of 2021. We received 65,700
            reports in January and 57,096 and 55,618 reports in February and
            March respectively.
          </p>
          <p>
            We also received <strong>21,200,699</strong> one-click reports for
            spam on 11,011,054 unique accounts during this reporting period.<br />‍
          </p>
          <h2>Enforcement Trend Analysis</h2>
          <h3>Child Safety</h3>
          <p>
            Discord has a zero-tolerance policy for anyone who endangers or
            sexualizes children. Users who upload abuse material of minors to
            Discord are reported to NCMEC and removed from the service. We
            deeply value our partnership with NCMEC and their efforts to ensure
            that grooming and endangerment cases are quickly escalated to law
            enforcement.&nbsp;
          </p>
          <p>
            In the first quarter of 2022, we reported
            <strong>10,695 accounts</strong> to NCMEC. 10,641 of those reports
            were media (images or videos), of which many were flagged through
            PhotoDNA - a tool which uses a shared industry hash database of
            known CSAM. 54 high-harm grooming or endangerment reports were also
            delivered to NCMEC. Overall this represented a 29% increase in
            reports made to NCMEC when compared to the fourth quarter of
            2021.&nbsp;
          </p>
          <figure>
            <div>
              <img src="../../images/child-safety.png" alt="" />
            </div>
            <figcaption>
              An accessible text version of the reports filed to NCMEC chart is
              available
              <a
                href="https://github.com/DiscordBlog/Discord-Transparency-Report-January---March-2022/blob/main/Q1%202022%20Transparency%20Report%20-%20NCMEC.csv"
                >on GitHub</a
              >.
            </figcaption>
          </figure>
          <p>
            Discord disabled <strong>826,591 accounts</strong> and removed
            <strong>24,706 servers</strong> for Child Safety in the first
            quarter of 2022. Sexualized Content Regarding Minors (SCRM) is the
            single largest individual sub-category of accounts disabled within
            Child Safety, accounting for
            <strong>718,385 accounts</strong> disabled and
            <strong>22,499 servers</strong> removed.
          </p>
          <p>
            Child-harm content is appalling, unacceptable, and has no place on
            Discord or the internet at large. We work with industry peers, civil
            society, and law enforcement to ensure that this effort extends
            beyond Discord. Discord is an active supporter of cross-industry
            programs such as NCMEC and is a member of the Technology Coalition.
            We’re also an annual sponsor of events dedicated to increasing
            awareness of and action on child safety issues such as the Dallas
            Crime Against Children Convention.
          </p>
          <p>
            We have a dedicated team and invest heavily in advanced tooling and
            <a
              href="https://discord.com/safety/360044154611-Talking-about-online-safety-with-your-teen"
              >education</a
            >
            so parents know how our service works and understand the controls
            that can contribute to creating a positive and safe experience on
            Discord for their children. As part of our ongoing commitment to
            parent engagement, Discord is a proud sponsor of National Parent
            Teacher Association and ConnectSafely. In recognition of Safer
            Internet Day in February, Discord and National PTA hosted an
            <a
              href="https://discord.com/blog/discord-pta-collaboration-announcement-educators-parents"
              >event</a
            >
            to bring together parents, educators and teens to discuss online
            safety. We continue to be a member of the Family Online Safety
            Institute, contributing to and learning from its important work.<br />‍
          </p>
          <h3>Deceptive Practices</h3>
          <p>
            Using Discord for the purpose of distributing malware, sharing or
            selling game hacks or cheats, and theft of authentication tokens is
            a violation of our Community Guidelines.&nbsp;
          </p>
          <p>
            We disabled <strong>5,091 accounts</strong> and removed
            <strong>1,797 servers</strong> for Deceptive Practices during the
            first quarter of 2022.<br />‍
          </p>
          <h3>Exploitative and Unsolicited Content</h3>
          <p>
            It is a violation of our Community Guidelines to share or promote
            sexually explicit content of other people without their consent.
          </p>
          <p>
            We disabled <strong>146,897 accounts</strong> and removed
            <strong>3,525 servers </strong>for Exploitative and Unsolicited
            Content. This category was the second-largest category of accounts
            disabled. These numbers were similar to the previous quarter, with
            one notable trend being that accounts disabled for Non-Consensual
            Pornography increased by 132% as a result of increased proactive
            efforts to remove this content from Discord.<br />‍
          </p>
          <h3>Exploitative and Unsolicited Content</h3>
          <p>
            It is a violation of our Community Guidelines to share or promote
            sexually explicit content of other people without their consent.
          </p>
          <p>
            We disabled <strong>146,897 accounts</strong> and removed
            <strong>3,525 servers </strong>for Exploitative and Unsolicited
            Content. This category was the second-largest category of accounts
            disabled. These numbers were similar to the previous quarter, with
            one notable trend being that accounts disabled for Non-Consensual
            Pornography increased by 132% as a result of increased proactive
            efforts to remove this content from Discord.<br />‍
          </p>
          <h3>Harassment and Bullying</h3>
          <p>
            Harassment and bullying have no place on Discord. Continuous,
            repetitive, or severe negative comments, circumventing bans,
            suggestive or overt threats, the sharing of someone’s personally
            identifiable information (also known as doxxing), and server raiding
            are violations of our Community Guidelines.
          </p>
          <p>
            During the first quarter of 2022,
            <strong>13,423 accounts</strong> were disabled for
            harassment-related behavior, and <strong>716</strong>
            <strong>servers</strong> were removed for this issue. Both the
            number of accounts disabled and servers removed stayed roughly
            consistent with the previous quarter.&nbsp;<br />‍
          </p>
          <h3>Hateful Conduct</h3>
          <p>
            Discord doesn’t allow the organization, promotion, or participation
            in hate speech or hateful conduct.
          </p>
          <p>
            During the first quarter of 2022,<strong> 8,806 accounts</strong>
            and <strong>965</strong> <strong>servers</strong> were removed for
            Hateful Conduct. Compared to the previous quarter, this was a
            decrease of 6.5% for accounts and 18% for servers.<br />‍
          </p>
          <h3>Identity and Authenticity</h3>
          <p>
            Using Discord for the purpose of coordinating and participating in
            malicious impersonation of individuals or organizations is a
            violation of our Community Guidelines.&nbsp;
          </p>
          <p>
            We disabled<strong> 154 accounts</strong> and removed
            <strong>16</strong> <strong>servers</strong> for this issue.<br />‍
          </p>
          <h3>Platform Manipulation</h3>
          <p>
            Spam, fake accounts, and self-bots are examples of platform
            manipulation that damage the experience of our users and violate our
            Community Guidelines.
          </p>
          <p>
            During the first quarter of 2022,
            <strong>2,905 accounts</strong> and <strong>972</strong>
            <strong>servers</strong> were removed for platform
            manipulation-related issues not related to spam. An additional
            <strong>26,017,742 accounts</strong> were disabled for spam or
            spam-related offenses.&nbsp;
          </p>
          <p>
            We’re focused on combating spam and minimizing users’ exposure to
            spammers and spam content on Discord. We recently established a
            dedicated cross-functional anti-spam team combining Engineering,
            Data, Product, and Safety resources to tackle this issue, and we
            rolled out a one-click spam reporting feature that enables users to
            easily report spam. In the first quarter of 2022, 90% of accounts
            disabled for spam were disabled proactively – before we received any
            user report.
          </p>
          <p>
            You can read more about how Discord fights spam
            <a href="https://discord.com/blog/how-discord-is-fighting-spam"
              >here</a
            >.&nbsp;<br />‍
          </p>
          <h3>Regulated or Illegal Activities</h3>
          <p>
            Using Discord for the purpose of engaging in regulated, illegal, or
            dangerous activities is strictly prohibited, including selling or
            facilitating the sale of prohibited or potentially dangerous goods
            or services.
          </p>
          <p>
            We disabled <strong>19,669 accounts</strong> for engaging in this
            behavior.
          </p>
          <p>
            A total of <strong>4,027 servers</strong> were removed for this
            category. Cybercrime had a proactive take down rate of 51%,
            with<strong> 1,996</strong> of the 3,883 servers removed
            proactively.<br />‍
          </p>
          <h3>Self-Harm Concerns</h3>
          <p>
            Using Discord to glorify or promote suicide or self-harm is not
            allowed under any circumstance.&nbsp;
          </p>
          <p>
            Actions may be taken on accounts or servers encouraging people to
            cut themselves or embrace eating disorders, or otherwise
            manipulating and coercing other users to engage in acts of
            self-harm. These actions are only taken on accounts glorifying or
            promoting acts of self-harm, not on users seeking help or in need of
            medical attention.
          </p>
          <p>
            We disabled <strong>1,795 accounts</strong> and removed
            <strong>594 servers</strong> for Self-Harm concerns.<br />‍
          </p>
          <h3>Violent and Graphic Content</h3>
          <p>
            Real media depicting gore, excessive violence, the glorification of
            violence, or animal cruelty with the intent to harass or shock
            others is not allowed on Discord.
          </p>
          <p>
            In the first quarter of 2022, <strong>16,069 accounts </strong>were
            disabled for posting violent and graphic content. 5,419 of these
            accounts were disabled for gore, and the remaining 10,650 accounts
            were disabled for content glorifying violence.&nbsp; This was a 34%
            decrease in accounts disabled for gore and a 64% increase in the
            number of accounts disabled for content glorifying violence.
          </p>
          <p>
            We also removed <strong>1,657 servers </strong>for violent and
            graphic content. Of these, 291 servers were removed for gore and
            1,366 servers were removed for content glorifying violence. The
            number of servers removed for gore decreased by 22% while the number
            of servers removed for glorification of violence increased by
            89%.<br />‍
          </p>
          <h3>Violent Extremism</h3>
          <p>
            We consider violent extremism to be the support, encouragement,
            promotion, or organization of violent acts or ideologies that
            advocate for the destruction of society, often by blaming certain
            individuals or groups and calling for violence against them.&nbsp;
          </p>
          <p>
            <a
              href="https://discord.com/blog/how-trust-safety-addresses-violent-extremism-on-discord"
              >This blog post</a
            >
            discusses our methods to address violent extremism. Through
            partnering and engaging in cross-industry work with Tech Against
            Terrorism, the Global Internet Forum To Counter Terrorism (GIFCT),
            the European Union Internet Forum and other organizations, we’ve
            made progress in our tooling, policy, and subject matter expertise
            to ensure that violent extremism does not have a home on
            Discord.&nbsp;
          </p>
          <p>
            In the first quarter of 2022, <strong>12,928 accounts</strong> and
            <strong>1,034 servers</strong> were removed for violent extremism.
            This reflects a 9% increase in the number of servers removed since
            the previous quarter. We dedicate significant resources to
            proactively detecting and removing violent extremism. During this
            quarter, 40.5% of servers removed for Violent Extremism were removed
            proactively.<br />‍
          </p>
          <h2>Information Requests</h2>
          <p>
            When appropriate, Discord complies with information requests from
            law enforcement agencies while respecting the privacy and rights of
            our users.
          </p>
          <p>
            Law enforcement must provide valid legal documentation for the
            requested information. We review each information request to ensure
            legal compliance.&nbsp;
          </p>
          <p>
            Discord may also disclose user data to law enforcement in emergency
            situations when we possess a good faith belief that there is an
            imminent risk of serious physical harm. You can read more about how
            Discord works with law enforcement
            <a
              href="https://discord.com/safety/360044157931-Working-with-law-enforcement"
              >here</a
            >.<br />‍
          </p>
          <h3>Local Requests</h3>
          <p>
            Discord received <strong>1,446 </strong>pieces<strong> </strong>of
            legal process during the first quarter of 2022. We complied with the
            majority of these requests, ultimately finding
            <strong>1,296</strong> both legally valid and specific enough for us
            to identify an account and produce the relevant information. We work
            to limit disclosures of user information and content so they match
            the specific circumstances dictated by each request.&nbsp;
          </p>
          <figure>
            <div>
              <img src="../../images/legal-requests.png" alt="" />
            </div>
            <figcaption>
              An accessible text version of the requests from law enforcement
              chart is available
              <a
                href="https://github.com/DiscordBlog/Discord-Transparency-Report-January---March-2022/blob/main/Q1%202022%20Transparency%20Report%20-%20Law%20Enforcement.csv"
                >on GitHub</a
              >.
            </figcaption>
          </figure>
          <h3>Emergency Requests</h3>
          <p>
            Discord received <strong>182 </strong>emergency disclosure requests
            from law enforcement during this period. These requests originated
            from law enforcement agencies around the world. We disclose user
            data to law enforcement absent legal process only when there is
            imminent risk of serious physical injury or death. We were able to
            identify 145 accounts based on the information provided by law
            enforcement for these requests and disclosed basic subscriber
            information in response to <strong>82</strong> emergency disclosure
            requests.&nbsp;<br />‍
          </p>
          <h2>Intellectual Property Removal Requests</h2>
          <p>
            Our Community Guidelines and Terms of Service prohibit the sharing
            of content that infringes third-party intellectual property rights.
            In accordance with the Digital Millennium Copyright Act (DMCA),
            Discord will remove content and/or accounts in connection with
            claims of copyright infringement on Discord.
          </p>
          <p>
            We review each DMCA notice to ensure that reports are valid and
            complete and that they are made in good faith.
          </p>
          <p>
            Discord received <strong>816 </strong>facially valid DMCA takedown
            notifications, of which <strong>813 </strong>provided information
            sufficient for content removal upon review.&nbsp;<br />‍
          </p>
          <h2>Our Commitment to Safety and Transparency</h2>
          <p>
            Imagine a place where you can talk, hang out with your friends, and
            find belonging in a welcoming and safe environment: it doesn’t just
            happen. It takes immense effort, collaboration, and dedication from
            all of us at Discord. We strive to build the best service for people
            around the world.&nbsp;
          </p>
          <p>
            We’re proud of the work that we do to help keep people and
            communities on Discord safe. We hope that as a result of this work,
            you’re able to find belonging in the communities on Discord that you
            call home.&nbsp;
          </p>
          <p>
            We hope that
            <a href="https://discord.com/tags/transparency-reports"
              >these Transparency Reports</a
            >, alongside our
            <a href="https://discord.com/category/safety"
              >Policy &amp; Safety Blog</a
            >
            will continue to educate and inspire others toward building a safer
            digital world. We’re committed to continuously providing more useful
            insight into this effort and welcome your feedback on how to improve
            these reports.
          </p>
        </div>
      </div>
      <div class="blog__divider-bar"></div>
      <div class="author-info__container">
        <div class="author-info__label">The Author</div>
        <div class="author-info__img-name-details-container">
          <div>
            <img
              class="author-info__img"
              src="../../images/discord-safety.png"
              alt=""
            />
          </div>
          <div class="author-info__name-details-container">
            <div class="author-info__name">Discord Safety</div>
            <div class="author-info__details">
              The Safety team works to ensure that Discord is a great place to
              find belonging in the communities you call home.
            </div>
          </div>
        </div>
      </div>
      <div class="blog__divider-bar"></div>
      <div class="comment-box">
        <h2>Comments</h2>
        <form class="comment-form">
          <input type="text" name="name" placeholder="Full Name" required />
          <textarea
            name="comment"
            placeholder="Write a comment.."
            required
          ></textarea>
          <button type="submit">Post Comment</button>
        </form>
      </div>
      <ul class="comments"></ul>
      <div class="blog__divider-bar"></div>
      <div class="more-from">
        <div class="more-from__title-category-container">
          <div class="more-from__title">More From</div>
          <div class="more-from__category">Policy & Safety</div>
        </div>
        <div class="more-from__blogs">
          <div class="more-from__blog">
            <div>
              <img
                class="more-from__blog-img"
                src="../../images/policy-and-safety-blog-2.png"
                alt=""
              />
            </div>
            <div class="more-from__categoy-title-container">
              <a class="more-from__blog-category" href="">Policy & Safety</a>
              <a class="more-from__blog-title" href=""
                >Discord Transparency Report: July – December 2021
              </a>
            </div>
          </div>
          <div class="more-from__blog">
            <div>
              <img
                class="more-from__blog-img"
                src="../../images/policy-and-safety-blog-4.png"
                alt=""
              />
            </div>
            <div class="more-from__categoy-title-container">
              <a class="more-from__blog-category" href="">Policy & Safety</a>
              <a class="more-from__blog-title" href=""
                >Addressing Health Misinformation
              </a>
            </div>
          </div>
          <div class="more-from__blog">
            <div>
              <img
                class="more-from__blog-img"
                src="../../images/policy-and-safety-blog-5.png"
                alt=""
              />
            </div>
            <div class="more-from__categoy-title-container">
              <a class="more-from__blog-category" href="">Policy & Safety</a>
              <a class="more-from__blog-title" href=""
                >Important Policy Updates
              </a>
            </div>
          </div>
        </div>
      </div>
    </main>
    <footer class="footer">
      <div class="footer-nav-socials__container">
        <div class="footer__socials">
          <h2 class="footer__title">Imagine a place</h2>
          <div class="footer__social-links">
            <a class="footer__social-link" href="">
              <img src="../../images/twitter.svg" alt="" />
            </a>
            <a class="footer__social-link" href="">
              <img src="../../images/instagram.svg" alt="" />
            </a>
            <a class="footer__social-link" href="">
              <img src="../../images/facebook.svg" alt="" />
            </a>
            <a class="footer__social-link" href="">
              <img src="../../images/youtube.svg" alt="" />
            </a>
          </div>
        </div>
        <nav class="footer-nav">
          <div class="footer-nav__link-group">
            <ul class="footer-nav__links">
              <div>Product</div>
              <li><a href="">Download</a></li>
              <li><a href="">Why Discord</a></li>
              <li><a href="">Inspiration</a></li>
              <li><a href="">College</a></li>
              <li><a href="">Nitro</a></li>
              <li><a href="">Status</a></li>
            </ul>
            <ul class="footer-nav__links">
              <div>Company</div>
              <li><a href="">Download</a></li>
              <li><a href="">Why Discord</a></li>
              <li><a href="">Inspiration</a></li>
              <li><a href="">College</a></li>
              <li><a href="">Nitro</a></li>
              <li><a href="">Status</a></li>
            </ul>
          </div>
          <div class="footer-nav__link-group">
            <ul class="footer-nav__links">
              <div>Resources</div>
              <li><a href="">Support</a></li>
              <li><a href="">Safety</a></li>
              <li><a href="">Blog</a></li>
              <li><a href="">Feedback</a></li>
              <li><a href="">Partners</a></li>
              <li><a href="">Verification</a></li>
              <li><a href="">Developers</a></li>
              <li><a href="">StreamKit</a></li>
              <li><a href="">Open Source</a></li>
              <li><a href="">Security</a></li>
              <li><a href="">Moderation</a></li>
            </ul>
            <ul class="footer-nav__links">
              <div>Policies</div>
              <li><a href="">Terms</a></li>
              <li><a href="">Privacy</a></li>
              <li><a href="">Guidelines</a></li>
              <li><a href="">Acknowledgments</a></li>
              <li><a href="">Licenses</a></li>
            </ul>
          </div>
        </nav>
      </div>
      <div class="footer__divider"></div>
      <div class="footer__bar">
        <a href="">
          <img src="../../images/discord-footer.svg" alt="" />
        </a>
        <a class="footer__download-btn" href="">Download</a>
        <a class="footer__open-discord-btn" href="">Open Discord</a>
      </div>
    </footer>
    <script src="../../scripts/comments.js"></script>
  </body>
</html>
